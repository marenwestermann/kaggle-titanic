{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival prediction of titanic passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# ML tools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#local modules\n",
    "from barplot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Plot the Figures Inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Prevent label cut off from figures \n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "meta_data = pd.read_csv(\"data/metadata.csv\")\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_data = pd.read_csv(\"data/titanic-train.csv\")\n",
    "print(\"Shape: \", train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data = pd.read_csv(\"data/titanic-test.csv\")\n",
    "print(\"Shape: \", test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the frequency of classes in the predicted variable (label) in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_deceased = (train_data[\"Survived\"] == 0).sum()\n",
    "num_survived = (train_data[\"Survived\"] == 1).sum()\n",
    "assert num_deceased + num_survived == 891\n",
    "print(\"deceased total: \", num_deceased, \" - deceased %: \", round(100/891*num_deceased, 1))\n",
    "print(\"survived total: \", num_survived, \" - deceased %: \", round(100/891*num_survived, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the number of fatalities is much higher than the number of survivors. This means that the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the datasets contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame({'Training set': train_data.isna().sum(), \n",
    "                               'Test set': test_data.isna().sum()})\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: There are many missing values for the age of passengers and the cabin type. Therefore, these features will be excluded from the following analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of unique values of features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Sex\"].nunique()\n",
    "train_data[\"SibSp\"].nunique()\n",
    "train_data[\"Parch\"].nunique()\n",
    "train_data[\"Fare\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: there are many different fares that are assumably associated with the ticket class. Let's check this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check min and max prices of fares per class\n",
    "\n",
    "# divide training dataset per class\n",
    "class1 = train_data.loc[train_data['Pclass'] == 1]\n",
    "class2 = train_data.loc[train_data['Pclass'] == 2]\n",
    "class3 = train_data.loc[train_data['Pclass'] == 3]\n",
    "# save classes in list\n",
    "classes = [class1, class2, class3]\n",
    "\n",
    "# print fare ranges\n",
    "for i, pclass in enumerate(classes):    \n",
    "    print(f\"Max fare class {i+1}: \", pclass[\"Fare\"].max())\n",
    "    print(f\"Min fare class {i+1}: \",pclass[\"Fare\"].min())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fares per class as histograms\n",
    "\n",
    "# save fares in numpy array\n",
    "fares_per_class = [class1[\"Fare\"].to_numpy(), \n",
    "             class2[\"Fare\"].to_numpy(), \n",
    "             class3[\"Fare\"].to_numpy()]\n",
    "\n",
    "# plot fares\n",
    "fig, ax = plt.subplots(1,len(fares_per_class), figsize=(15, 5))\n",
    "for i, data in enumerate(fares_per_class):\n",
    "    _ = ax[i].hist(data, bins=20)\n",
    "    _ = ax[i].set_title(f\"Class {i+1}\")\n",
    "    _ = ax[i].set_xlabel(\"Fares\")\n",
    "    _ = ax[i].set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The fares of the 3 different classes overlap, especially the fares of class 2 and 3. It might therefore be more useful to predict survival rates depending on passenger class rather than fare. Let's check among the categorical features if there are categories that are (strongly) associated with survival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate survival rates per categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categories in list and convert them to string variables for plotting\n",
    "categories_class = list(map(str, train_data[\"Pclass\"].unique()))\n",
    "categories_class.sort()\n",
    "categories_class # check result\n",
    "\n",
    "# calculate percentage of survivors per passenger class\n",
    "survivors_per_class = []\n",
    "for pclass in classes: \n",
    "    surv = round(pclass[\"Survived\"].sum()/len(pclass[\"Survived\"])*100, 1)\n",
    "    survivors_per_class.append(surv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per class\n",
    "plot_survivors_per_category(categories_class, \n",
    "             survivors_per_class, \n",
    "             title=\"Survivors per passenger class\", \n",
    "             xlabel=\"Passenger classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to total survivors per passenger class\n",
    "train_data[\"Pclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the survival rate seems to be correlated to the passenger class and therefore likely influences the prediction of survival. Although the survival rate is highest for passengers in class 1, most people travelled in class 3.\n",
    "\n",
    "#### Gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categories in list\n",
    "categories_gender = list(map(str, train_data[\"Sex\"].unique()))\n",
    "categories_gender # check result\n",
    "\n",
    "# calculate percentage of survivors per gender\n",
    "men = train_data.loc[train_data.Sex == 'male'][\"Survived\"].to_numpy()\n",
    "women = train_data.loc[train_data.Sex == 'female'][\"Survived\"].to_numpy()\n",
    "men_surv = round(sum(men)/len(men)*100, 1)\n",
    "women_surv = round(sum(women)/len(women)*100, 1)\n",
    "\n",
    "# store results in list\n",
    "survivors_per_gender = [men_surv, women_surv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per gender\n",
    "plot_survivors_per_category(categories_gender, \n",
    "             survivors_per_gender, \n",
    "             title=\"Survivors per gender\", \n",
    "             xlabel=\"Gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to total survivors per gender\n",
    "train_data[\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the survival rate of women is much higher than the survival rate of men although the number of men aboard the Titanic was much higher compared to the number of women. Therefore, the gender likely has a strong influence on the prediction of survival.\n",
    "\n",
    "#### Number of siblings/ spouses aboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categories in list\n",
    "categories_sibsp = list(train_data[\"SibSp\"].unique())\n",
    "categories_sibsp.sort()\n",
    "\n",
    "# calculate percentage of survivors per number of siblings/ spouses aboard\n",
    "# and save results in list\n",
    "survivors_per_sibsp = []\n",
    "for i in categories_sibsp:\n",
    "    sibsp = train_data.loc[train_data.SibSp == i][\"Survived\"].to_numpy()\n",
    "    survivors_per_sibsp.append(round(sum(sibsp)/len(sibsp)*100, 1))\n",
    "\n",
    "# convert categories to string variables for plotting\n",
    "categories_sibsp = list(map(str, categories_sibsp))\n",
    "categories_sibsp # check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per number of siblings/ spouses aboard\n",
    "plot_survivors_per_category(categories_sibsp, \n",
    "             survivors_per_sibsp, \n",
    "             title=\"Survivors per number of siblings/ spouses aboard\", \n",
    "             xlabel=\"Number of siblings/ spouses aboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to total survivors per siblings/ spouses aboard\n",
    "train_data[\"SibSp\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The people with 1 or 2 siblings/ spouses aboard had the highest rate of survival. This could mean that these people had support from family members with getting a spot in one of the lifeboats. However, the number of people who travelled with no siblings or spouses is more than twice as high as the number of people who travelled in company. Therefore, the number of siblings/ spouses might be weakly associated with the chance of survival.\n",
    "\n",
    "#### Number of parents/ children aboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categories in list\n",
    "categories_parch = list(train_data[\"Parch\"].unique())\n",
    "categories_parch.sort()\n",
    "\n",
    "# calculate percentage of survivors per number of parents/ children aboard\n",
    "# and save results in list\n",
    "survivors_per_parch = []\n",
    "for i in categories_parch:\n",
    "    parch = train_data.loc[train_data.Parch == i][\"Survived\"].to_numpy()\n",
    "    survivors_per_parch.append(round(sum(parch)/len(parch)*100, 1))\n",
    "\n",
    "# convert categories to string variables for plotting\n",
    "categories_parch = list(map(str, categories_parch))\n",
    "categories_parch # check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per number of parents/ children aboard\n",
    "plot_survivors_per_category(categories_parch, \n",
    "             survivors_per_parch, \n",
    "             title=\"Survivors per number of parents/ children aboard\", \n",
    "             xlabel=\"Number of parents/ children aboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to total survivors per parents/ children aboard\n",
    "train_data[\"Parch\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The people who had between 1 and 3 parents/ children aboard had the highest rate of survival. As above, this could mean that these people had support from family members with getting a spot in one of the lifeboats. However, most people travelled without company. Therefore, the number of parents/ children aboard might be weakly associated with the chance of survival.\n",
    "\n",
    "#### Port of embarkation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categories in list\n",
    "categories_embarked = list(map(str, train_data[\"Embarked\"].unique()))\n",
    "categories_embarked\n",
    "\n",
    "# calculate percentage of survivors per port of embarkation\n",
    "# note: leave out the two passengers of unknown port of embarkation\n",
    "survivors_per_port = []\n",
    "for i in categories_embarked[:3]:\n",
    "    port = train_data.loc[train_data.Embarked == i][\"Survived\"].to_numpy()\n",
    "    survivors_per_port.append(round(sum(port)/len(port)*100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per port of embarkation\n",
    "plot_survivors_per_category(categories_embarked[:3], \n",
    "             survivors_per_port, \n",
    "             title=\"Survivors per port of embarkation\", \n",
    "             xlabel=\"Port of embarkation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to total survivors per port of embarkation\n",
    "train_data[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Although by far the most people embarked in Southampton the percentage of survivors who embarked in Cherbourg is higher compared to Southampton and Queenstown. This could be due to many first class passengers having embarked here. Let's check this: \n",
    "\n",
    "#### Passengers per class per port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of survivors per class and port of embarkation\n",
    "# note: leave out the two passengers of unknown port of embarkation\n",
    "\n",
    "survivors_class_port = []\n",
    "# loop over classes\n",
    "for pclass in classes: \n",
    "    survivors_per_port_pclass = []\n",
    "    # loop over ports\n",
    "    for cat in categories_embarked[:3]:\n",
    "        port = pclass.loc[pclass.Embarked == cat][\"Survived\"].to_numpy().sum()\n",
    "        survivors_per_port_pclass.append(port)\n",
    "    survivors_class_port.append(survivors_per_port_pclass)\n",
    "\n",
    "survivors_class_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot survivors per class per port of embarkation\n",
    "\n",
    "# set variables\n",
    "x = np.arange(len(categories_embarked[:3]))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.set_title(\"Survivors per class per port\")\n",
    "_ = ax.set_ylabel(\"Survivors (toal)\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels(categories_embarked[:3])\n",
    "\n",
    "# plot barplot\n",
    "for i,j in zip(survivors_class_port,range(-1,2)): \n",
    "    _ = ax.bar(x=x+width*j, height=i, width=width, label=f'Class {j+2}')\n",
    "    # annotate barplot\n",
    "    for k, data in enumerate(i):\n",
    "        _ = ax.annotate(s=data, xy=(k+width*j, data+0.7), ha='center')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Most survivors, irrespective of class, embarked in Southampton. However, in Cherbourg a higher number of survivors belonging to the first class embarked compared to second and thrid class survivors. Additionally, in Queenstown a higher number of survivors belonging to the third class embarked compared to first and second class survivors. Therefore, the port of embarkation might have a weak influence on the prediction of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Based on this data exploration, the features that likely influence the prediction of survival are in presumed descending order of strength: \n",
    "- gender\n",
    "- passenger class\n",
    "- siblings/ spouses aboard; children/ parents aboard; port of embarkation/ fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation\n",
    "\n",
    "### Select features to be included in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and label column\n",
    "features = [\"Sex\", \"Pclass\", \"SibSp\", \"Parch\", \"Embarked\", \"Survived\"] #\n",
    "\n",
    "# create pruned dataset\n",
    "train_data_pruned = train_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values and check result\n",
    "X_pruned = train_data_pruned.dropna()\n",
    "X_pruned.shape\n",
    "X_pruned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in separate variable\n",
    "y = X_pruned['Survived']\n",
    "X = X_pruned.drop(columns=['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features as ordinal integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "enc.fit(X)\n",
    "X_ord = enc.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models\n",
    "\n",
    "We now train different classic machine learning models on the training set and evaluate their performance with k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run different classic machine learning models: \n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- Logistic Regression\n",
    "- Suport Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0, n_jobs=-1, class_weight={0:2, 1:1})\n",
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "lr = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr', max_iter=100, class_weight={0:2, 1:1})\n",
    "svm = LinearSVC(random_state=0, C=1.0, max_iter=1000, class_weight={0:2, 1:1})\n",
    "\n",
    "# save models in list\n",
    "models = [rf, gb, lr, svm]\n",
    "# save model names in list\n",
    "model_names = ['RF', 'GB', 'LR', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform cross validation\n",
    "accuracies = []\n",
    "std = []\n",
    "for model, names in zip(models, model_names):\n",
    "    scores = cross_val_score(model, X_ord, y, cv=5)\n",
    "    accuracies.append(scores.mean())\n",
    "    std.append(scores.std())\n",
    "    print(f\"Cross val scores {names}: \", np.around(scores, decimals=2))\n",
    "    print(f\"Mean and stdev: {scores.mean():.2f} +/- {scores.std():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies of different ML models\n",
    "plot_model_accuracies(model_names, accuracies, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the performances of the different classifiers are in a narrow range with mean accuracies between 78% and 80%, and standard deviations between 1% and 3%. In order to better understand the models, let's have a look into model explainability by investigating the importances of individual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check feature importances\n",
    "#### Calculate the impurity-based feature importances of the tree classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "_ = rf.fit(X_ord, y)\n",
    "importances_rf = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting classifier\n",
    "_ = gb.fit(X_ord, y)\n",
    "importances_gb = gb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store values in list\n",
    "importances_trees = [importances_rf.tolist(), importances_gb.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the impurity-based feature importances of the tree classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_names = [\"Random forest\", \"Gradient boosting\"] # classifier names\n",
    "\n",
    "plot_feature_importance(clf=clf_names, \n",
    "                        feat=features, \n",
    "                        ylabel=\"Importance scores\",\n",
    "                        importances=importances_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the feature importance scores are almost identical between the random forest classifier and the gradient boosting classifier. By far the most important feature is gender followed by passenger class. As postulated above, the features siblings/ spouses aboard, parents/ children aboard and port of embaraktion play a minor role in survival prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weights assigned to the features in the linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "_ = lr.fit(X_ord, y)\n",
    "importances_lr = lr.coef_.ravel()*-1 # flatten array and make weights positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector machine\n",
    "_ = svm.fit(X_ord, y)\n",
    "importances_svm = svm.coef_.ravel()*-1 # flatten array and make weights positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store values in list\n",
    "importances_linear = [importances_lr.tolist(), importances_svm.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot weights assigned to the features in the linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_names = [\"Logistic Regression\", \"Support Vector Machine\"] # classifier names\n",
    "\n",
    "plot_feature_importance(clf=clf_names, \n",
    "                        feat=features, \n",
    "                        ylabel=\"Feature weights\",\n",
    "                        importances=importances_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: overall, feature weights are higher in the logistic regression model compared to the support vector machine model. However, the relative importances between features within the models are roughly equal. As observed in the random forest model and the gradient boosting model, the most important feature is gender followed by passenger class while the features siblings/ spouses aboard, parents/ children aboard and port of embaraktion play a minor role in survival prediction. It is notable that the feature parents/ children aboard seems to be of less importance in the linear models compared to the tree classifiers. <br>\n",
    "Note: the feature weights are negative but I turned them into positive numbers in order to make them more comparable to the importance scores of the tree classifiers. In binary classification negative feature weights indicate that a feature tends more towards predicting 0. Because the dataset is imbalanced towards fatalities (see data exploration part), feature weights are negative.\n",
    "\n",
    "### Perform hyperparameter tuning\n",
    "\n",
    "The hyperparameter settings for the models might not be optimal. Let's do a hyperparameter grid search to find out if the hyperparameter settings can be optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifiers and store them in list\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "svm = LinearSVC()\n",
    "\n",
    "classifiers = [rf, gb, lr, svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of classifiers and store them in list\n",
    "parameters_rf = {'n_estimators':[50, 100, 200], 'max_depth':[None, 5, 10]}\n",
    "parameters_gb = {'learning_rate':[0.05, 0.1, 0.5], 'n_estimators':[50, 100, 200], 'max_depth':[None, 5, 10]}\n",
    "parameters_lr = {'solver':['newton-cg', 'lbfgs', 'liblinear'], 'max_iter':[100, 500, 1000]}\n",
    "parameters_svm = {'max_iter':[1000, 3000, 5000]}\n",
    "\n",
    "parameters = [parameters_rf, parameters_gb, parameters_lr, parameters_svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform best parameter grid search per classifier and print results\n",
    "for est, params, name in zip(classifiers, parameters, model_names): \n",
    "    clf = GridSearchCV(est, params, cv=5, n_jobs=-1)\n",
    "    _ = clf.fit(X_ord, y)  \n",
    "    \n",
    "    # show results\n",
    "    print(\"Model: \", name)\n",
    "    print(\"Scores per params combination: \", np.around(clf.cv_results_['mean_test_score'], decimals=2))\n",
    "    print(\"Best score: \", round(clf.best_score_, 2))\n",
    "    print(\"Best parameter combination: \", clf.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: after performing hyperparameter tuning, the model accuracies are almost identical (± 1%) compared to the model accuracies previous to hyperparameter tuning. This means 1) that the hyperparameter settings that were initially chosen were likely (close to) optimal and 2) that all tested models show a certain robustness against changing hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
